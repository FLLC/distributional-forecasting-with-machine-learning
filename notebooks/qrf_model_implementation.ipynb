{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e59c11aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import warnings\n",
    "\n",
    "# Data science / numeric\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import make_scorer\n",
    "from scipy.stats import kstest\n",
    "\n",
    "# Probabilistic scoring\n",
    "from properscoring import crps_gaussian, crps_ensemble\n",
    "\n",
    "# PyTorch (keep if you use it later)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Utilities\n",
    "import joblib\n",
    "import optuna\n",
    "\n",
    "# Suppress warnings in notebook output\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0999d2ce",
   "metadata": {},
   "source": [
    "# 0) Importing Data and Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6b9bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "# Import data\n",
    "#############################################################################\n",
    "x_test = pd.read_csv('X_test.csv')\n",
    "x_trn = pd.read_csv('X_trn.csv')\n",
    "y_trn = pd.read_csv('y_trn.csv')\n",
    "\n",
    "xy_trn = x_trn.copy()\n",
    "xy_trn.insert(0, 'realrinc', y_trn.squeeze())\n",
    "xy_trn = xy_trn.sort_values(by='year').reset_index(drop=True) # sorted by year\n",
    "\n",
    "x_test_transformed = x_test.copy() # will transform below \n",
    "\n",
    "#############################################################################\n",
    "# Preprocessing TRAIN DATA \n",
    "#############################################################################\n",
    "df = xy_trn.copy()\n",
    "\n",
    "### TARGET variable: \"realrinc\" ###\n",
    "# Apply log(1 + y) to reduce skewness\n",
    "# Then standardize\n",
    "df[\"realrinc\"] = np.log1p(df[\"realrinc\"])\n",
    "mean_y = df[\"realrinc\"].mean()\n",
    "std_y  = df[\"realrinc\"].std()\n",
    "df[\"realrinc\"] = (df[\"realrinc\"] - mean_y) / std_y\n",
    "\n",
    "### REMAINING features ###\n",
    "## AGE\n",
    "mean_age = df[\"age\"].mean()\n",
    "std_age  = df[\"age\"].std()\n",
    "df[\"age\"] = (df[\"age\"] - mean_age) / std_age\n",
    "\n",
    "## pretg10\n",
    "mean_prestg10 = df[\"prestg10\"].mean()\n",
    "std_prestg10  = df[\"prestg10\"].std()\n",
    "df[\"prestg10\"] = (df[\"prestg10\"] - mean_prestg10) / std_prestg10\n",
    "\n",
    "## Childs\n",
    "# Apply log(1 + x) \n",
    "# Then standardize\n",
    "df[\"childs\"] = np.log1p(df[\"childs\"])\n",
    "mean_childs = df[\"childs\"].mean()\n",
    "std_childs  = df[\"childs\"].std()\n",
    "df[\"childs\"] = (df[\"childs\"] - mean_childs) / std_childs\n",
    "\n",
    "## Gender \n",
    "df[\"gender\"] = np.where(df[\"gender\"] == \"Male\", 1, 0)\n",
    "\n",
    "## Marital Status (one-hot encoding)\n",
    "df = pd.get_dummies(df, columns=[\"maritalcat\"], drop_first=False)\n",
    "marital_cols = [col for col in df.columns if col.startswith(\"maritalcat_\")]\n",
    "df[marital_cols] = df[marital_cols].astype(int)\n",
    "\n",
    "## Work Status (one-hot encoding)\n",
    "df = pd.get_dummies(df, columns=[\"wrkstat\"], drop_first=False)\n",
    "wrkstat_cols = [col for col in df.columns if col.startswith(\"wrkstat_\")]\n",
    "df[wrkstat_cols] = df[wrkstat_cols].astype(int)\n",
    "\n",
    "## Occupation (one-hot encoding)\n",
    "df = pd.get_dummies(df, columns=[\"occrecode\"], drop_first=False)\n",
    "occrecode_cols = [col for col in df.columns if col.startswith(\"occrecode_\")]\n",
    "df[occrecode_cols] = df[occrecode_cols].astype(int)\n",
    "\n",
    "## Education (ordinal encodigng)\n",
    "edu_map = {\n",
    "    \"Less Than High School\": 0,\n",
    "    \"High School\": 1,\n",
    "    \"Junior College\": 2,\n",
    "    \"Bachelor\": 3,\n",
    "    \"Graduate\": 4\n",
    "}\n",
    "df[\"educcat\"] = df[\"educcat\"].map(edu_map)\n",
    "\n",
    "### STORE: KEEP TRACK FOR LATER ###\n",
    "scalers = {\n",
    "    \"realrinc\": {\"mean\": mean_y, \"std\": std_y, \"log_transform\": True},\n",
    "    \"age\": {\"mean\": mean_age, \"std\": std_age, \"log_transform\": False},\n",
    "    \"prestg10\": {\"mean\": mean_prestg10, \"std\": std_prestg10, \"log_transform\": False},\n",
    "    \"childs\": {\"mean\": mean_childs, \"std\": std_childs, \"log_transform\": True}\n",
    "}\n",
    "\n",
    "# Separating X/y\n",
    "y_trn_new = df[\"realrinc\"].copy()\n",
    "x_trn_new = df.drop(columns=[\"realrinc\"]).copy()\n",
    "\n",
    "#############################################################################\n",
    "# Preprocessing TEST DATA\n",
    "#############################################################################\n",
    "df = x_test.copy()\n",
    "\n",
    "### FEATURES ###\n",
    "\n",
    "## AGE — use training mean/std\n",
    "df[\"age\"] = (df[\"age\"] - scalers[\"age\"][\"mean\"]) / scalers[\"age\"][\"std\"]\n",
    "\n",
    "## prestg10 — use training mean/std\n",
    "df[\"prestg10\"] = (df[\"prestg10\"] - scalers[\"prestg10\"][\"mean\"]) / scalers[\"prestg10\"][\"std\"]\n",
    "\n",
    "## Childs — log + standardize using training mean/std\n",
    "df[\"childs\"] = np.log1p(df[\"childs\"])\n",
    "df[\"childs\"] = (df[\"childs\"] - scalers[\"childs\"][\"mean\"]) / scalers[\"childs\"][\"std\"]\n",
    "\n",
    "## Gender — same binary encoding as train\n",
    "df[\"gender\"] = np.where(df[\"gender\"] == \"Male\", 1, 0)\n",
    "\n",
    "## Marital Status (one-hot encoding)\n",
    "df = pd.get_dummies(df, columns=[\"maritalcat\"], drop_first=False)\n",
    "marital_cols = [col for col in df.columns if col.startswith(\"maritalcat_\")]\n",
    "df[marital_cols] = df[marital_cols].astype(int)\n",
    "\n",
    "## Work Status (one-hot encoding)\n",
    "df = pd.get_dummies(df, columns=[\"wrkstat\"], drop_first=False)\n",
    "wrkstat_cols = [col for col in df.columns if col.startswith(\"wrkstat_\")]\n",
    "df[wrkstat_cols] = df[wrkstat_cols].astype(int)\n",
    "\n",
    "## Occupation (one-hot encoding)\n",
    "df = pd.get_dummies(df, columns=[\"occrecode\"], drop_first=False)\n",
    "occrecode_cols = [col for col in df.columns if col.startswith(\"occrecode_\")]\n",
    "df[occrecode_cols] = df[occrecode_cols].astype(int)\n",
    "\n",
    "## Education (ordinal encoding)\n",
    "edu_map = {\n",
    "    \"Less Than High School\": 0,\n",
    "    \"High School\": 1,\n",
    "    \"Junior College\": 2,\n",
    "    \"Bachelor\": 3,\n",
    "    \"Graduate\": 4\n",
    "}\n",
    "df[\"educcat\"] = df[\"educcat\"].map(edu_map)\n",
    "\n",
    "# Align columns with training data \n",
    "df = df.reindex(columns=x_trn_new.columns, fill_value=0)\n",
    "\n",
    "# Copy the result to x_test_transformed\n",
    "x_test_transformed = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ebabaa",
   "metadata": {},
   "source": [
    "# 1) Define the Model (QRF) and helper functions that are used throughout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700b680c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _weighted_quantile(x, q, w):\n",
    "    x = np.asarray(x); w = np.asarray(w)\n",
    "    s = np.argsort(x); x, w = x[s], w[s]\n",
    "    cw = np.cumsum(w) / np.sum(w)\n",
    "    return np.interp(q, cw, x)\n",
    "\n",
    "class QRF:\n",
    "    \"\"\"\n",
    "    Quantile Regression Forest using leaf-weight aggregation over training targets.\n",
    "    - predict_quantiles(): weighted quantiles of training y\n",
    "    - sample(): draws from empirical conditional distribution using leaf weights\n",
    "    \"\"\"\n",
    "    def __init__(self, **rf_params):\n",
    "        self.model = RandomForestRegressor(**rf_params)\n",
    "        self.leaf_maps_ = None\n",
    "        self.y_train_ = None\n",
    "        self.n_train_ = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = pd.DataFrame(X).reset_index(drop=True)\n",
    "        y = pd.Series(y).reset_index(drop=True).values\n",
    "        self.model.fit(X, y)\n",
    "        self.y_train_ = y\n",
    "        self.n_train_ = len(y)\n",
    "\n",
    "        # For each tree: map leaf_id -> indices of training samples in that leaf\n",
    "        self.leaf_maps_ = []\n",
    "        for tree in self.model.estimators_:\n",
    "            leaves = tree.apply(X)\n",
    "            m = {}\n",
    "            for lid in np.unique(leaves):\n",
    "                m[lid] = np.where(leaves == lid)[0]\n",
    "            self.leaf_maps_.append(m)\n",
    "        return self\n",
    "\n",
    "    def _weights_for(self, Xq):\n",
    "        Xq = pd.DataFrame(Xq)\n",
    "        n_q = len(Xq)\n",
    "        W = np.zeros((n_q, self.n_train_), dtype=float)\n",
    "        n_trees = len(self.model.estimators_)\n",
    "        for t, tree in enumerate(self.model.estimators_):\n",
    "            lids = tree.apply(Xq)\n",
    "            mp = self.leaf_maps_[t]\n",
    "            for i, lid in enumerate(lids):\n",
    "                idx = mp[lid]\n",
    "                W[i, idx] += 1.0 / (n_trees * len(idx))\n",
    "        # each row sums to 1\n",
    "        return W\n",
    "\n",
    "    def predict_quantiles(self, X, quantiles):\n",
    "        qs = np.asarray(quantiles).ravel()\n",
    "        W = self._weights_for(X)\n",
    "        out = np.empty((len(X), len(qs)), dtype=float)\n",
    "        for i in range(len(X)):\n",
    "            out[i] = [_weighted_quantile(self.y_train_, q, W[i]) for q in qs]\n",
    "        return out\n",
    "\n",
    "    def sample(self, X, n_samples=1000, rng=None):\n",
    "        if rng is None:\n",
    "            rng = np.random.default_rng(1738)\n",
    "        W = self._weights_for(X)\n",
    "        draws = np.empty((len(X), n_samples), dtype=float)\n",
    "        for i in range(len(X)):\n",
    "            draws[i] = rng.choice(self.y_train_, size=n_samples, p=W[i])\n",
    "        return draws\n",
    "\n",
    "def crps_from_samples(y_true, samples):\n",
    "    y_true = np.asarray(y_true).reshape(-1, 1)\n",
    "    samples = np.asarray(samples)\n",
    "    n, S = samples.shape\n",
    "    A = np.mean(np.abs(samples - y_true), axis=1)\n",
    "    xs = np.sort(samples, axis=1)\n",
    "    k = np.arange(1, S + 1, dtype=float)\n",
    "    weights = 2 * k - S - 1\n",
    "    B = (2.0 / (S**2)) * np.sum(xs * weights, axis=1)\n",
    "    return A - 0.5 * B\n",
    "\n",
    "def _predict_quantiles_any(model, X, alphas):\n",
    "    for attr in [\"predict_quantiles\", \"predict\", \"quantile\", \"predict_quantile\"]:\n",
    "        if hasattr(model, attr):\n",
    "            try:\n",
    "                q = getattr(model, attr)(X, alphas)\n",
    "            except TypeError:\n",
    "                q = getattr(model, attr)(X, q=alphas)\n",
    "            q = np.asarray(q)\n",
    "            if q.ndim == 1 and len(alphas) == 1:\n",
    "                q = q.reshape(-1, 1)\n",
    "            if q.shape[1] == len(alphas):\n",
    "                return q\n",
    "    raise NotImplementedError(\"No quantile method found.\")\n",
    "\n",
    "def inverse_cdf_sample_from_quantiles(qvals, alphas, n_samples, rng):\n",
    "    qvals = np.asarray(qvals)\n",
    "    alphas = np.asarray(alphas).ravel()\n",
    "    n, m = qvals.shape\n",
    "    U = rng.uniform(alphas[0], alphas[-1], size=(n, n_samples))\n",
    "    out = np.empty((n, n_samples), dtype=float)\n",
    "    for i in range(n):\n",
    "        out[i] = np.interp(U[i], alphas, qvals[i])\n",
    "    return out\n",
    "\n",
    "def get_predictive_samples(model, X, n_samples=200, rng=None, alphas=None):\n",
    "    if rng is None:\n",
    "        rng = np.random.default_rng(1738)\n",
    "    if alphas is None:\n",
    "        alphas = np.linspace(0.01, 0.99, 199)\n",
    "    if hasattr(model, \"sample\"):\n",
    "        try:\n",
    "            draws = np.asarray(model.sample(X, n_samples=n_samples))\n",
    "            if draws.shape == (len(X), n_samples):\n",
    "                return draws\n",
    "        except Exception:\n",
    "            pass\n",
    "    qvals = _predict_quantiles_any(model, X, alphas)\n",
    "    return inverse_cdf_sample_from_quantiles(qvals, alphas, n_samples, rng)\n",
    "\n",
    "def cv_crps_for_params(X, y, qrf_cls, params, K=5, S=200, seed=1738):\n",
    "    if not isinstance(X, (pd.DataFrame, pd.Series)):\n",
    "        X = pd.DataFrame(X)\n",
    "    y = pd.Series(y).reset_index(drop=True)\n",
    "    kf = KFold(n_splits=K, shuffle=True, random_state=seed)\n",
    "    rng = np.random.default_rng(seed)\n",
    "    scores = []\n",
    "    for tr, va in kf.split(X):\n",
    "        Xtr, Xva = X.iloc[tr], X.iloc[va]\n",
    "        ytr, yva = y.iloc[tr], y.iloc[va]\n",
    "        model = qrf_cls(**params)\n",
    "        model.fit(Xtr, ytr)\n",
    "        samples = get_predictive_samples(model, Xva, n_samples=S, rng=rng)\n",
    "        scores.append(np.mean(crps_from_samples(yva.values, samples)))\n",
    "    return float(np.mean(scores))\n",
    "\n",
    "def sample_qrf_params(rng):\n",
    "    n_estimators = int(rng.choice([50, 200, 400, 800])) \n",
    "    max_depth = int(rng.integers(4, 20))            \n",
    "    min_samples_leaf = int(rng.integers(2, 10))     \n",
    "    mf_choice = rng.choice(['sqrt', 'log2'])\n",
    "    return dict(\n",
    "        n_estimators=n_estimators,\n",
    "        max_depth=max_depth,\n",
    "        min_samples_leaf=min_samples_leaf,\n",
    "        max_features=mf_choice,\n",
    "        bootstrap=True,\n",
    "        random_state=1738,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "\n",
    "def random_search_qrf(X, y, qrf_cls, n_trials=60, K=5, S=200, seed=1738, verbose=False):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    recs, best = [], {\"score\": np.inf, \"params\": None}\n",
    "    for t in range(1, n_trials + 1):\n",
    "        params = sample_qrf_params(rng)\n",
    "        score = cv_crps_for_params(X, y, qrf_cls, params, K=K, S=S, seed=seed)\n",
    "        recs.append({**params, \"cv_crps\": score})\n",
    "        if score < best[\"score\"]:\n",
    "            best = {\"score\": score, \"params\": params}\n",
    "        if verbose:\n",
    "            print(f\"[{t}/{n_trials}] CV-CRPS={score:.6f} | best={best['score']:.6f}\")\n",
    "    df = pd.DataFrame.from_records(recs).sort_values(\"cv_crps\")\n",
    "    return df, best\n",
    "\n",
    "def fit_best_and_write_predictions(X, y, X_test, qrf_cls, best_params, n_samples=1000, seed=1738):\n",
    "    if not isinstance(X, (pd.DataFrame, pd.Series)):\n",
    "        X = pd.DataFrame(X)\n",
    "    if not isinstance(X_test, (pd.DataFrame, pd.Series)):\n",
    "        X_test = pd.DataFrame(X_test)\n",
    "    model = qrf_cls(**best_params)\n",
    "    model.fit(X, y)\n",
    "    rng = np.random.default_rng(seed)\n",
    "    draws = get_predictive_samples(model, X_test, n_samples=n_samples, rng=rng)\n",
    "    return model, draws\n",
    "\n",
    "def inverse_transform(y_scaled, mean_y, std_y, log_transform=True):\n",
    "    y_unscaled = y_scaled * std_y + mean_y\n",
    "    return np.expm1(y_unscaled) if log_transform else y_unscaled\n",
    "\n",
    "def compute_pit(y_true, samples):\n",
    "    \"\"\"\n",
    "    Compute Probability Integral Transform (PIT) values\n",
    "    given samples from predictive distribution.\n",
    "    \"\"\"\n",
    "    # For each observation, compute fraction of samples <= actual value\n",
    "    return np.mean(samples <= y_true[:, None], axis=1)\n",
    "\n",
    "def gini(x):\n",
    "    x = np.sort(np.asarray(x))\n",
    "    n = len(x)\n",
    "    cumx = np.cumsum(x)\n",
    "    return (n + 1 - 2 * np.sum(cumx) / cumx[-1]) / n\n",
    "\n",
    "def sample_income_qrf(year, gender, n_samples=1000, rng=None):\n",
    "    \"\"\"\n",
    "    Draw income samples from QRF model for a given year and gender.\n",
    "    - year: numeric (e.g., 2000)\n",
    "    - gender: 0 for Female, 1 for Male\n",
    "    \"\"\"\n",
    "    Xq = pd.DataFrame({\n",
    "        \"year\": [year],   \n",
    "        \"gender\": [gender]  \n",
    "    })\n",
    "    log_draws = qrf.sample(Xq, n_samples=n_samples, rng=rng).ravel()\n",
    "    return np.expm1(log_draws)  # Back-transform from log income\n",
    "\n",
    "def prob_male_greater(year, n=1000, rng=None):\n",
    "    m = sample_income_qrf(year, 1, n_samples=n, rng=rng)\n",
    "    f = sample_income_qrf(year, 0, n_samples=n, rng=rng)\n",
    "    return float(np.mean(m > f)), m, f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29476e7d",
   "metadata": {},
   "source": [
    "# 2) Model training/hyperparameter opt via k-fold CV\n",
    "- 2.1) Random Search for HOP via 5-fold CV\n",
    "- 2.2) Train model on a 80/20 split to compute the CRPS value \n",
    "- 2.3) Train model on all of the data and get predictions for X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe84c6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.1) Hyperparameter tuning\n",
    "# FOR QRF\n",
    "X = x_trn_new.copy()\n",
    "y = y_trn_new.copy()\n",
    "\n",
    "results_df, best = random_search_qrf(\n",
    "    X, y, QRF, n_trials=20, K=5, S=200, seed=1738, verbose=True\n",
    ")\n",
    "\n",
    "# We store the best hyperparameters found by the random search\n",
    "# Uncomment to use best params from random search\n",
    "#best_params = best[\"params\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45c3bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best hyperparameters found (tuned manually after random search)\n",
    "best_params = {\n",
    "    'n_estimators': 800,\n",
    "    'max_depth': 18,\n",
    "    'min_samples_leaf': 4,\n",
    "    'max_features': \"sqrt\",\n",
    "    'bootstrap': True,\n",
    "    'random_state': 1738,  # synced with your random search seed\n",
    "    'n_jobs': -1\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1b2f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2) CRPS for 1 single validation set + PIT Plot\n",
    "X = x_trn_new.copy()\n",
    "y = y_trn_new.copy()\n",
    "\n",
    "# Split original training set into new training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=1738\n",
    ")\n",
    "\n",
    "# Fit model on 80% of original training set\n",
    "final_model_crps = QRF(**best_params).fit(X_train, y_train)\n",
    "\n",
    "# Prediction samples for 20% validation set\n",
    "samples_z = get_predictive_samples(final_model_crps, X_val, n_samples=1000)\n",
    "\n",
    "# --- Inverse transform both samples and targets ---\n",
    "mean_y = scalers[\"realrinc\"][\"mean\"]\n",
    "std_y  = scalers[\"realrinc\"][\"std\"]\n",
    "log_transform = scalers[\"realrinc\"][\"log_transform\"]\n",
    "\n",
    "y_val_orig = inverse_transform(y_val, mean_y, std_y, log_transform)\n",
    "samples_orig = inverse_transform(samples_z, mean_y, std_y, log_transform)\n",
    "\n",
    "# --- Compute CRPS in $ ---\n",
    "crps_values = crps_from_samples(y_val_orig, samples_orig)\n",
    "mean_crps_dollars = np.mean(crps_values)\n",
    "print(f\"Mean CRPS on original scale: {mean_crps_dollars:,.2f} USD\")\n",
    "\n",
    "#######################################################################################\n",
    "# PIT PLOT\n",
    "#######################################################################################\n",
    "\n",
    "# Assuming y_val_orig and samples_orig are in the ORIGINAL scale\n",
    "pit_values = compute_pit(np.asarray(y_val_orig), np.asarray(samples_orig))\n",
    "\n",
    "# KS STAT\n",
    "ks_stat, ks_p = kstest(pit_values, 'uniform')\n",
    "print(f\"KS statistic = {ks_stat:.3f}, p-value = {ks_p:.3f}\")\n",
    "\n",
    "# Plot PIT histogram\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.hist(pit_values, bins=20, range=(0,1), edgecolor=\"black\", density=True)\n",
    "plt.axhline(1, color=\"red\", linestyle=\"--\", label=\"Uniform(0,1)\")\n",
    "plt.title(\"PIT Histogram – QRF model\")\n",
    "plt.xlabel(\"PIT value\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cb8ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.3) Train model on all of the data + get predictions for X_test\n",
    "model, draws_scaled = fit_best_and_write_predictions(\n",
    "    X, y, x_test_transformed,\n",
    "    qrf_cls=QRF,\n",
    "    best_params=best_params,\n",
    "    n_samples=1000,\n",
    "    seed=1738\n",
    ")\n",
    "\n",
    "mean_y = scalers[\"realrinc\"][\"mean\"]\n",
    "std_y  = scalers[\"realrinc\"][\"std\"]\n",
    "log_transform = scalers[\"realrinc\"][\"log_transform\"]\n",
    "\n",
    "# Revert back to original Scale for predictions\n",
    "draws_unscaled = inverse_transform(\n",
    "    draws_scaled,\n",
    "    mean_y=scalers[\"realrinc\"][\"mean\"],\n",
    "    std_y=scalers[\"realrinc\"][\"std\"],\n",
    "    log_transform=scalers[\"realrinc\"][\"log_transform\"]\n",
    ")\n",
    "\n",
    "np.save(\"predictions.npy\", draws_unscaled)\n",
    "print(\"Saved predictions.npy with shape:\", draws_unscaled.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7496d447",
   "metadata": {},
   "source": [
    "# 3) Gender Subquestions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175996cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1) Train Model\n",
    "# Load data to ensure correctness\n",
    "x_trn = pd.read_csv(\"X_trn.csv\")\n",
    "y_trn = pd.read_csv(\"y_trn.csv\").squeeze()\n",
    "\n",
    "data = x_trn[[\"year\", \"gender\"]].copy()\n",
    "data[\"gender\"] = np.where(data[\"gender\"] == \"Male\", 1, 0)\n",
    "data[\"log_income\"] = np.log1p(y_trn)\n",
    "\n",
    "X = data[[\"year\", \"gender\"]]\n",
    "y = data[\"log_income\"]\n",
    "\n",
    "# We fit a QRF with sensible defaults given only 2 features \n",
    "# and lack of time to tune\n",
    "qrf = QRF(\n",
    "    n_estimators=800,\n",
    "    max_depth=20,\n",
    "    min_samples_leaf=5,\n",
    "    max_features=\"sqrt\",\n",
    "    bootstrap=True,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ").fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8eb0700",
   "metadata": {},
   "outputs": [],
   "source": [
    " # 3.2) Subquestion 1\n",
    "rng = np.random.default_rng(1738)\n",
    "male_1980   = sample_income_qrf(1980, 1, 1000, rng=rng)\n",
    "female_1980 = sample_income_qrf(1980, 0, 1000, rng=rng)\n",
    "\n",
    "res_1980 = {\n",
    "    \"Male_STD\":   float(np.std(male_1980, ddof=1)),\n",
    "    \"Female_STD\": float(np.std(female_1980, ddof=1)),\n",
    "    \"Male_Gini\":  gini(male_1980),\n",
    "    \"Female_Gini\":gini(female_1980),\n",
    "}\n",
    "\n",
    "# PRINT RESULTS \n",
    "print(\"\\n=== Income Inequality (1980, QRF) ===\")\n",
    "print(f\"Male   → Std: {res_1980['Male_STD']:.2f},  Gini: {res_1980['Male_Gini']:.3f}\")\n",
    "print(f\"Female → Std: {res_1980['Female_STD']:.2f},  Gini: {res_1980['Female_Gini']:.3f}\")\n",
    "\n",
    "# Histogram \n",
    "plt.figure(figsize=(4,3))\n",
    "plt.hist(male_1980, bins=30, alpha=0.6, density=True, label=\"Male\", color=\"steelblue\", edgecolor=\"black\")\n",
    "plt.hist(female_1980, bins=30, alpha=0.6, density=True, label=\"Female\", color=\"orange\", edgecolor=\"black\")\n",
    "\n",
    "plt.title(\"Income Distribution by Gender (1980)\", fontsize=9)\n",
    "plt.xlabel(\"Real Income (USD)\", fontsize=8)\n",
    "plt.ylabel(\"Density\", fontsize=8)\n",
    "plt.xticks(fontsize=7)\n",
    "plt.yticks(fontsize=7)\n",
    "plt.legend(fontsize=7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27472d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.3) Subquestion 2\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# 1980 samples \n",
    "male_1980   = sample_income_qrf(1980, 1, n_samples=1000, rng=rng)\n",
    "female_1980 = sample_income_qrf(1980, 0, n_samples=1000, rng=rng)\n",
    "\n",
    "diff_1980 = male_1980 - female_1980\n",
    "prob_male_gt_female_1980 = np.mean(diff_1980 > 0)\n",
    "\n",
    "# 2010 samples\n",
    "male_2010   = sample_income_qrf(2010, 1, n_samples=1000, rng=rng)\n",
    "female_2010 = sample_income_qrf(2010, 0, n_samples=1000, rng=rng)\n",
    "\n",
    "diff_2010 = male_2010 - female_2010\n",
    "prob_male_gt_female_2010 = np.mean(diff_2010 > 0)\n",
    "\n",
    "# PRINT RESULTS \n",
    "print(\"\\n=== Probability that Male Earned More than Female ===\")\n",
    "print(f\"1980 → P(Male > Female): {prob_male_gt_female_1980:.3f}\")\n",
    "print(f\"2010 → P(Male > Female): {prob_male_gt_female_2010:.3f}\")\n",
    "\n",
    "# Plot histogram for 1980 \n",
    "plt.figure(figsize=(4,3))\n",
    "plt.hist(diff_1980, bins=30, color=\"steelblue\", edgecolor=\"black\", density=True)\n",
    "plt.axvline(0, color=\"red\", linestyle=\"--\", label=\"Equal income\")\n",
    "plt.title(\"Predicted Income Differences (1980)\", fontsize=9)\n",
    "plt.xlabel(\"Income difference (USD)\", fontsize=8)\n",
    "plt.ylabel(\"Density\", fontsize=8)\n",
    "plt.xticks(fontsize=7)\n",
    "plt.yticks(fontsize=7)\n",
    "plt.legend(fontsize=7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot histogram for 2010\n",
    "plt.figure(figsize=(4,3))\n",
    "plt.hist(diff_2010, bins=30, color=\"orange\", edgecolor=\"black\", density=True)\n",
    "plt.axvline(0, color=\"red\", linestyle=\"--\", label=\"Equal income\")\n",
    "plt.title(\"Predicted Income Differences (2010)\", fontsize=9)\n",
    "plt.xlabel(\"Income difference (USD)\", fontsize=8)\n",
    "plt.ylabel(\"Density\", fontsize=8)\n",
    "plt.xticks(fontsize=7)\n",
    "plt.yticks(fontsize=7)\n",
    "plt.legend(fontsize=7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "959119af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot male vs female for 1980\n",
    "plt.figure(figsize=(4,3))\n",
    "plt.hist(male_1980 / 1000, bins=30, alpha=0.6, label=\"Male\",\n",
    "         color=\"steelblue\", edgecolor=\"black\", density=True)\n",
    "plt.hist(female_1980 / 1000, bins=30, alpha=0.6, label=\"Female\",\n",
    "         color=\"orange\", edgecolor=\"black\", density=True)\n",
    "plt.title(\"Predicted Income Distributions (1980)\", fontsize=9)\n",
    "plt.xlabel(\"Income (thousands of USD)\", fontsize=8)\n",
    "plt.ylabel(\"Density\", fontsize=8)\n",
    "plt.legend(fontsize=7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot male vs female for 2010\n",
    "plt.figure(figsize=(4,3))\n",
    "plt.hist(male_2010 / 1000, bins=30, alpha=0.6, label=\"Male\",\n",
    "         color=\"steelblue\", edgecolor=\"black\", density=True)\n",
    "plt.hist(female_2010 / 1000, bins=30, alpha=0.6, label=\"Female\",\n",
    "         color=\"orange\", edgecolor=\"black\", density=True)\n",
    "plt.title(\"Predicted Income Distributions (2010)\", fontsize=9)\n",
    "plt.xlabel(\"Income (thousands of USD)\", fontsize=8)\n",
    "plt.ylabel(\"Density\", fontsize=8)\n",
    "plt.legend(fontsize=7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
